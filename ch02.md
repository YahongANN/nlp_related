## 目录
[2. 词典分词](#2-词典分词)
-    [2.1 什么是词](#2-1)
-    [2.2 词典](#2-2)
-    [2.3 切分算法](#2-3)
-    [2.4 字典树](#2-4)
-    [2.5 基于字典树的其它算法](#2-5)
-    [2.6 HanLP的字典分词实现](#2-6)

## 2. 词典分词
- 中文分词：指的是将一段文字拆分为一系列单词的过程，这些单词顺序拼接后等于原文。
- 中文分词算法大致分为基于词典规则与基于起学习两大派
<span id='2-1'></span>

### 2.1 什么是词
- 在基于词典的中文分词中，词的定义要显示的多：词典中的字符串就是词。
- 词的性质--齐夫定律：一个单词的词频与它的词频排名成反比。
<span id='2-2'></span>
### 2.2 词典
开源的词库例如：[sougow15万个词条](https://www.sogou.com/labs/resource/w.php?_blank)，清华大学开放中文词库 [THUOCL](http://thuocl.thunlp.org), HanLP词库(千万级词条)
<span id='2-3'></span>
### 2.3 切分算法
首先，加载词典

```
def load_dictionary():
    dic = set()

    # 按行读取字典文件，每行第一个空格之前的字符串提取出来。
    for line in open("CoreNatureDictionary.mini.txt","r"):
        dic.add(line[0:line.find('	')])
    
    return dic
```
1. 完全切分

指的是，找出一段文本中的所有单词

```
def fully_segment(text, dic):
    word_list = []
    for i in range(len(text)):                  # i 从 0 到text的最后一个字的下标遍历
        for j in range(i + 1, len(text) + 1):   # j 遍历[i + 1, len(text)]区间
            word = text[i:j]                    # 取出连续区间[i, j]对应的字符串
            if word in dic:                     # 如果在词典中，则认为是一个词
                word_list.append(word)
    return word_list
  
dic = load_dictionary()
print(fully_segment('就读北京大学', dic))
```
输出

```
['就', '就读', '读', '北', '北京', '北京大学', '京', '大', '大学', '学']
```
输出了所有可能的单词。由于词库中含有单字，所以结果中也出现了一些单字。

2. 正向最长匹配
[pythontutor.com可视化代码示例](http://www.pythontutor.com/visualize.html#code=def%20forward_segment%28text,%20dic%29%3A%0A%20%20%20%20word_list%20%3D%20%5B%5D%0A%20%20%20%20i%20%3D%200%0A%20%20%20%20while%20i%20%3C%20len%28text%29%3A%0A%20%20%20%20%20%20%20%20longest_word%20%3D%20text%5Bi%5D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20%E5%BD%93%E5%89%8D%E6%89%AB%E6%8F%8F%E4%BD%8D%E7%BD%AE%E7%9A%84%E5%8D%95%E5%AD%97%0A%20%20%20%20%20%20%20%20for%20j%20in%20range%28i%20%2B%201,%20len%28text%29%20%2B%201%29%3A%20%20%20%20%20%20%20%23%20%E6%89%80%E6%9C%89%E5%8F%AF%E8%83%BD%E7%9A%84%E7%BB%93%E5%B0%BE%0A%20%20%20%20%20%20%20%20%20%20%20%20word%20%3D%20text%5Bi%3Aj%5D%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20%E4%BB%8E%E5%BD%93%E5%89%8D%E4%BD%8D%E7%BD%AE%E5%88%B0%E7%BB%93%E5%B0%BE%E7%9A%84%E8%BF%9E%E7%BB%AD%E5%AD%97%E7%AC%A6%E4%B8%B2%0A%20%20%20%20%20%20%20%20%20%20%20%20if%20word%20in%20dic%3A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20%E5%9C%A8%E8%AF%8D%E5%85%B8%E4%B8%AD%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20if%20len%28word%29%20%3E%20len%28longest_word%29%3A%20%20%20%23%20%E5%B9%B6%E4%B8%94%E6%9B%B4%E9%95%BF%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20longest_word%20%3D%20word%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20%E5%88%99%E6%9B%B4%E4%BC%98%E5%85%88%E8%BE%93%E5%87%BA%0A%20%20%20%20%20%20%20%20word_list.append%28longest_word%29%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20%E8%BE%93%E5%87%BA%E6%9C%80%E9%95%BF%E8%AF%8D%0A%20%20%20%20%20%20%20%20i%20%2B%3D%20len%28longest_word%29%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%23%20%E6%AD%A3%E5%90%91%E6%89%AB%E6%8F%8F%0A%20%20%20%20return%20word_list%0A%0Adic%20%3D%20set%28%5B'%E5%B0%B1','%E8%AF%BB','%E5%B0%B1%E8%AF%BB','%E5%8C%97','%E4%BA%AC','%E5%8C%97%E4%BA%AC','%E5%A4%A7','%E5%AD%A6','%E5%8C%97%E4%BA%AC%E5%A4%A7%E5%AD%A6','%E7%A0%94','%E7%A9%B6','%E7%94%9F','%E7%A0%94%E7%A9%B6','%E7%A0%94%E7%A9%B6%E7%94%9F','%E5%91%BD','%E7%94%9F%E5%91%BD','%E8%B5%B7','%E6%BA%90','%E8%B5%B7%E6%BA%90'%5D%29%0Aprint%28forward_segment%28'%E5%B0%B1%E8%AF%BB%E5%8C%97%E4%BA%AC%E5%A4%A7%E5%AD%A6',%20dic%29%29%0Aprint%28forward_segment%28'%E7%A0%94%E7%A9%B6%E7%94%9F%E5%91%BD%E8%B5%B7%E6%BA%90',%20dic%29%29&cumulative=false&curInstr=6&heapPrimitives=nevernest&mode=display&origin=opt-frontend.js&py=3&rawInputLstJSON=%5B%5D&textReferences=false)
```
def forward_segment(text, dic):
    word_list = []
    i = 0
    while i < len(text):
        longest_word = text[i]                      # 当前扫描位置的单字
        for j in range(i + 1, len(text) + 1):       # 所有可能的结尾
            word = text[i:j]                        # 从当前位置到结尾的连续字符串
            if word in dic:                         # 在词典中
                if len(word) > len(longest_word):   # 并且更长
                    longest_word = word             # 则更优先输出
        word_list.append(longest_word)              # 输出最长词
        i += len(longest_word)                      # 正向扫描
    return word_list

dic = load_dictionary()
print(forward_segment('就读北京大学', dic))
print(forward_segment('研究生命起源', dic))
```

输出

```
['就读', '北京大学']
['研究生', '命', '起源']
```
3. 逆向最长匹配


```
def backward_segment(text, dic):
    word_list = []
    i = len(text) - 1
    while i >= 0:                                   # 扫描位置作为终点
        longest_word = text[i]                      # 扫描位置的单字
        for j in range(0, i):                       # 遍历[0, i]区间作为待查询词语的起点
            word = text[j: i + 1]                   # 取出[j, i]区间作为待查询单词
            if word in dic:
                if len(word) > len(longest_word):   # 越长优先级越高
                    longest_word = word
                    break
        word_list.insert(0, longest_word)           # 逆向扫描，所以越先查出的单词在位置上越靠后
        i -= len(longest_word)
    return word_list

dic = load_dictionary()
print(backward_segment('研究生命起源', dic))
print(backward_segment('项目的研究', dic))
```

输出:

```
['研究', '生命', '起源']
['项', '目的', '研究']
```

4. 双向最长匹配

```
def count_single_char(word_list: list):  # 统计单字成词的个数
    return sum(1 for word in word_list if len(word) == 1)


def bidirectional_segment(text, dic):
    f = forward_segment(text, dic)
    b = backward_segment(text, dic)
    if len(f) < len(b):                                  # 词数更少优先级更高
        return f
    elif len(f) > len(b):
        return b
    else:
        if count_single_char(f) < count_single_char(b):  # 单字更少优先级更高
            return f
        else:
            return b                                     # 都相等时逆向匹配优先级更高
        

print(bidirectional_segment('研究生命起源', dic))
print(bidirectional_segment('项目的研究', dic))
```

输出:

```
['研究', '生命', '起源']
['项', '目的', '研究']
```
<span id='2-4'></span>
### 2.4 字典树

匹配算法的瓶颈之一在于如何判断词典中是否含有字符串。如果用有序集合TreeMap的话，复杂度是O(logn),n是词典的大小；如果用散列表（Java的HashMap，Python的dict的话），时间复杂度会下降，但是内存消耗就会上去。所以为了达到速度快，存储少的目的衍生出了**字典树**的数据结构。

1. 什么是字典树

字符串集合常用语字典树（trie树，前缀树）存储，这是一种字符串上的树形数据结构。字典树种每条边对应一个字，从根节点往下的路径构成一个个字符串。字典树并不直接在节点上存储字符串，而是将赐予视作根节点到某节点之间的一条路径，并在重点节点（蓝色）上做个标记“该节点对应赐予的结尾”。字符串就是一条路径，要查询一个单词，只需顺着这条路径从根节点往下走。如果能走到特殊标记的节点，则说明该字符串在集合中，否则说明不存在。一个典型的字典树如下图所示：

当词典大小为 n 时，虽然最坏情况下字典树的复杂度依然是O(logn) (假设子节点用对数复杂度的数据结构存储，所有词语都是单字)，但它的实际速度比二分查找快。这是因为随着路径的深入，前缀匹配是递进的过程，算法不必比较字符串的前缀。

2. 字典树的实现
  

每个节点都应该至少知道自己的节点与对应的边，以及自己是否对应一个词。如果要实现映射而不是集合的话，还需要知道自己对应的值。我们约定用值为None表示节点不对应赐予，下面是字典树的实现：

<span id='2-5'></span>
### 2.5 基于字典树的其它算法

字典树的数据结构在以上的切分算法中已经很快了，在此基础上作者又做了一些优化，把分词速度推向了千万字每秒的级别。
- 首字三列其余二分的字典树
- 双数组字典树
- AC自动机（多模式匹配）
- 基于双数组字典的AC自动机

<span id='2-6'></span>
### 2.6 HanLP的词典分词实现

1. DoubleArrayTrieSegment

DoubleArrayTrieSegment分词器是对DAT最长匹配的封装，默认加载hanlp.properties中CoreDictionaryPath指定的词典。

```
from pyhanlp import *

# 不显示词性
HanLP.Config.ShowTermNature = False

# 可传入自定义字典 [dir1, dir2]
segment = DoubleArrayTrieSegment()
# 激活数字和英文识别
segment.enablePartOfSpeechTagging(True)

print(segment.seg("江西鄱阳湖干枯，中国最大淡水湖变成大草原"))
print(segment.seg("上海市虹口区大连西路550号SISU"))
```

输出

```
[江西, 鄱阳湖, 干枯, ，, 中国, 最大, 淡水湖, 变成, 大草原]
[上海市, 虹口区, 大连, 西路, 550, 号, SISU]
```
2. 去掉停用词

```
def load_from_file(path):
    """
    从词典文件加载DoubleArrayTrie
    :param path: 词典路径
    :return: 双数组trie树
    """
    map = JClass('java.util.TreeMap')()  # 创建TreeMap实例
    with open(path) as src:
        for word in src:
            word = word.strip()  # 去掉Python读入的\n
            map[word] = word
    return JClass('com.hankcs.hanlp.collection.trie.DoubleArrayTrie')(map)


## 去掉停用词
def remove_stopwords_termlist(termlist, trie):
    return [term.word for term in termlist if not trie.containsKey(term.word)]


trie = load_from_file('stopwords.txt')
termlist = segment.seg("江西鄱阳湖干枯了，中国最大的淡水湖变成了大草原")
print('去掉停用词前：', termlist)

print('去掉停用词后：', remove_stopwords_termlist(termlist, trie))
```
输出

```
去掉停用词前： [江西, 鄱阳湖, 干枯, 了, ，, 中国, 最大, 的, 淡水湖, 变成, 了, 大草原]
去掉停用词后： ['江西', '鄱阳湖', '干枯', '中国', '最大', '淡水湖', '变成', '大草原']
```
